{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddabf73c",
   "metadata": {},
   "source": [
    "# Training a NN for metric on CICY with homog\n",
    "\n",
    "## Import the required packages/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a9cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pickle\n",
    "import sys\n",
    "#sys.path.append(\"/Users/kit/Documents/Phys_Working/MF metric\")\n",
    "#sys.path.append(\"/home/f/fraser-talientec/PhysicalYukawas\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout)\n",
    "\n",
    "from cymetric.pointgen.pointgen import PointGenerator\n",
    "from cymetric.pointgen.nphelper import prepare_dataset, prepare_basis_pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "from cymetric.models.tfmodels import PhiFSModel, MultFSModel, FreeModel, MatrixFSModel, AddFSModel, PhiFSModelToric, MatrixFSModelToric\n",
    "from cymetric.models.tfhelper import prepare_tf_basis, train_model\n",
    "from cymetric.models.callbacks import SigmaCallback, KaehlerCallback, TransitionCallback, RicciCallback, VolkCallback, AlphaCallback\n",
    "from cymetric.models.metrics import SigmaLoss, KaehlerLoss, TransitionLoss, RicciLoss, VolkLoss, TotalLoss\n",
    "\n",
    "from NewCustomMetrics import *\n",
    "from laplacian_funcs import *\n",
    "#from generate_and_train_all_nnsHOLO import *\n",
    "from custom_networks import *\n",
    "import sys\n",
    "import importlib\n",
    "from AlphaPrimeModel import *\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d00ecc1-f6f4-47d4-a2bb-f7830e164093",
   "metadata": {},
   "source": [
    "## Point Cloud Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba80cc",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d1dd93a",
   "metadata": {},
   "source": [
    "Set the properties of the defining polynomial. And the point in Kahler Moduli space\n",
    "\n",
    "If correct, this should be for the following defining polynomial\n",
    "$0.44 x_{1,0}^2 x_{3,0}^2 x_{4,0}^2 x_{2,0}^2+0.88 x_{1,1}^2 x_{3,0}^2 x_{4,0}^2 x_{2,0}^2+0.88 x_{1,0}^2 x_{3,1}^2 x_{4,0}^2 x_{2,0}^2+0.88 x_{1,1}^2 x_{3,1}^2 x_{4,0}^2 x_{2,0}^2-0.03 x_{1,0} x_{1,1} x_{3,0} x_{3,1} x_{4,0}^2 x_{2,0}^2+0.44 x_{1,0}^2 x_{3,0}^2 x_{4,1}^2 x_{2,0}^2+0.44 x_{1,1}^2 x_{3,0}^2 x_{4,1}^2 x_{2,0}^2+0.88 x_{1,0}^2 x_{3,1}^2 x_{4,1}^2 x_{2,0}^2+0.44 x_{1,1}^2 x_{3,1}^2 x_{4,1}^2 x_{2,0}^2-0.41 x_{1,0} x_{1,1} x_{3,0} x_{3,1} x_{4,1}^2 x_{2,0}^2-0.41 x_{1,0} x_{1,1} x_{3,0}^2 x_{4,0} x_{4,1} x_{2,0}^2-0.03 x_{1,0} x_{1,1} x_{3,1}^2 x_{4,0} x_{4,1} x_{2,0}^2+0.62 x_{1,0}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1} x_{2,0}^2+0.62 x_{1,1}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1} x_{2,0}^2-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,0}^2 x_{4,0}^2 x_{2,0}-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,1}^2 x_{4,0}^2 x_{2,0}+0.41 x_{1,0}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,0}^2 x_{2,0}+0.03 x_{1,1}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,0}^2 x_{2,0}-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,0}^2 x_{4,1}^2 x_{2,0}-0.62 x_{1,0} x_{1,1} x_{2,1} x_{3,1}^2 x_{4,1}^2 x_{2,0}+0.03 x_{1,0}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,1}^2 x_{2,0}+0.41 x_{1,1}^2 x_{2,1} x_{3,0} x_{3,1} x_{4,1}^2 x_{2,0}+0.41 x_{1,0}^2 x_{2,1} x_{3,0}^2 x_{4,0} x_{4,1} x_{2,0}+0.03 x_{1,1}^2 x_{2,1} x_{3,0}^2 x_{4,0} x_{4,1} x_{2,0}+0.03 x_{1,0}^2 x_{2,1} x_{3,1}^2 x_{4,0} x_{4,1} x_{2,0}+0.41 x_{1,1}^2 x_{2,1} x_{3,1}^2 x_{4,0} x_{4,1} x_{2,0}+0.9 x_{1,0} x_{1,1} x_{2,1} x_{3,0} x_{3,1} x_{4,0} x_{4,1} x_{2,0}+0.44 x_{1,0}^2 x_{2,1}^2 x_{3,0}^2 x_{4,0}^2+0.88 x_{1,1}^2 x_{2,1}^2 x_{3,0}^2 x_{4,0}^2+0.44 x_{1,0}^2 x_{2,1}^2 x_{3,1}^2 x_{4,0}^2+0.44 x_{1,1}^2 x_{2,1}^2 x_{3,1}^2 x_{4,0}^2-0.41 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,0} x_{3,1} x_{4,0}^2+0.88 x_{1,0}^2 x_{2,1}^2 x_{3,0}^2 x_{4,1}^2+0.88 x_{1,1}^2 x_{2,1}^2 x_{3,0}^2 x_{4,1}^2+0.88 x_{1,0}^2 x_{2,1}^2 x_{3,1}^2 x_{4,1}^2+0.44 x_{1,1}^2 x_{2,1}^2 x_{3,1}^2 x_{4,1}^2-0.03 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,0} x_{3,1} x_{4,1}^2-0.03 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,0}^2 x_{4,0} x_{4,1}-0.41 x_{1,0} x_{1,1} x_{2,1}^2 x_{3,1}^2 x_{4,0} x_{4,1}+0.62 x_{1,0}^2 x_{2,1}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1}+0.62 x_{1,1}^2 x_{2,1}^2 x_{3,0} x_{3,1} x_{4,0} x_{4,1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe185432",
   "metadata": {},
   "outputs": [],
   "source": [
    "monomialsTQ = 5*np.eye(5, dtype=np.int64)\n",
    "coefficientsTQ = np.ones(5)\n",
    "kmoduliTQ = np.ones(1)\n",
    "ambientTQ = np.array([4])\n",
    "nameofmanifold=\"Quintic\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_points_and_save_using_defaults(free_coefficient,number_points,force_generate=False,seed_set=0):\n",
    "   coefficients=coefficientsTQ\n",
    "   # coefficients=np.array([1, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, \\\n",
    "   # 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, free_coefficient, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1])\n",
    "   monomials=monomialsTQ\n",
    "   kmoduli=kmoduliTQ\n",
    "   ambient=ambientTQ\n",
    "   # monomials = 5*np.eye(5, dtype=np.int64)\n",
    "   # coefficients = np.ones(5)\n",
    "   \n",
    "   # kmoduli = np.ones(1)\n",
    "   # ambient = np.array([4])\n",
    "   pg = PointGenerator(monomials, coefficients, kmoduli, ambient)\n",
    "   pg._set_seed(seed_set)\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   print(\"dirname: \" + dirname)\n",
    "   #test if the directory exists, if not, create it\n",
    "   if force_generate or (not os.path.exists(dirname)):\n",
    "      print(\"Generating: forced? \" + str(force_generate))\n",
    "      kappa = pg.prepare_dataset(number_points, dirname)\n",
    "      pg.prepare_basis(dirname, kappa=kappa)\n",
    "   elif os.path.exists(dirname):\n",
    "      try:\n",
    "         print(\"loading prexisting dataset\")\n",
    "         data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "         if (len(data['X_train'])+len(data['X_val']))!=number_points:\n",
    "            print(\"wrong length - generating anyway\")\n",
    "            kappa = pg.prepare_dataset(number_points, dirname)\n",
    "            pg.prepare_basis(dirname, kappa=kappa)\n",
    "      except:\n",
    "         print(\"error loading - generating anyway\")\n",
    "         kappa = pg.prepare_dataset(number_points, dirname)\n",
    "         pg.prepare_basis(dirname, kappa=kappa)\n",
    "   \n",
    "\n",
    "def getcallbacksandmetrics(data):\n",
    "   #rcb = RicciCallback((data['X_val'], data['y_val']), data['val_pullbacks'])\n",
    "   scb = SigmaCallback((data['X_val'], data['y_val']))\n",
    "   volkcb = VolkCallback((data['X_val'], data['y_val']))\n",
    "   kcb = KaehlerCallback((data['X_val'], data['y_val']))\n",
    "   tcb = TransitionCallback((data['X_val'], data['y_val']))\n",
    "   #cb_list = [rcb, scb, kcb, tcb, volkcb]\n",
    "   cb_list = [ scb, kcb, tcb, volkcb]\n",
    "   cmetrics = [TotalLoss(), SigmaLoss(), KaehlerLoss(), TransitionLoss(), VolkLoss()]#, RicciLoss()]\n",
    "   return cb_list, cmetrics\n",
    "\n",
    "def train_and_save_nn(free_coefficient,nlayer=3,nHidden=128,nEpochs=50,stddev=0.1,bSizes=[192,50000],lRate=0.001,use_zero_network=False):\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   name = 'phimodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(bSizes[1]) + 's' + str(nlayer) + 'x' +str(nHidden)\n",
    "   print('dirname: ' + dirname)\n",
    "   print('name: ' + name)\n",
    "   \n",
    "   data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirname, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetrics(data)\n",
    "\n",
    "   act = 'gelu'\n",
    "   alpha = [1., 1., 30., 1., 2.] # 1 AND 3??\n",
    "   ambient=tf.cast(tf.math.abs(BASIS['AMBIENT']),tf.int32)\n",
    "\n",
    "   #nfirstlayer=tf.reduce_prod(2*(np.array(ambient)+1)).numpy().item()\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   nn_phi = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi=make_nn(10,1,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_phi_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   # print(nn_phi_zero(tf.cast(data['X_val'][0:2],tf.float32)))\n",
    "   #nn_phi_zero=make_nn(10,1,nlayer,nHidden,act,use_zero_network=True)\n",
    "   phimodel = PhiFSModel(nn_phi, BASIS, alpha=alpha)\n",
    "   phimodelzero = PhiFSModel(nn_phi_zero, BASIS, alpha=alpha)\n",
    "\n",
    "   #Note, currently running legacy due to ongoing tf issue with M1/M2. \n",
    "   #Use the commented line instead if not on an M1/M2 machine\n",
    "   opt = tfk.optimizers.Adam(learning_rate=lRate)\n",
    "   #opt = tfk.optimizers.legacy.Adam(learning_rate=lRate)\n",
    "   # compile so we can test on validation set before training\n",
    "   phimodel.compile(custom_metrics=cmetrics)\n",
    "   phimodelzero.compile(custom_metrics=cmetrics)\n",
    "\n",
    "   ## compare validation loss before training for zero network and nonzero network\n",
    "   datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   #need to re-enable learning, in case there's been a problem:\n",
    "   phimodel.learn_transition = False\n",
    "   phimodelzero.learn_transition = False\n",
    "   phimodel.learn_volk = True\n",
    "   phimodelzero.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   #phimodelzero.learn_ricci_val= True\n",
    "   valzero=phimodelzero.test_step(datacasted)\n",
    "   valraw=phimodel.test_step(datacasted)\n",
    "   # phimodel.learn_ricci_val=False \n",
    "   # phimodelzero.learn_ricci_val=False \n",
    "   valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   valraw = {key: value.numpy() for key, value in valraw.items()}\n",
    "\n",
    "   phimodel, training_history = train_model(phimodel, data, optimizer=opt, epochs=nEpochs, batch_sizes=bSizes, \n",
    "                                       verbose=1, custom_metrics=cmetrics, callbacks=cb_list)\n",
    "   print(\"finished training\\n\")\n",
    "   phimodel.model.save(os.path.join(dirname, name))\n",
    "   np.savez_compressed(os.path.join(dirname, 'trainingHistory-' + name),training_history)\n",
    "   #now print the initial losses and final losses for each metric\n",
    "   # first_metrics = {key: value[0] for key, value in training_history.items()}\n",
    "   # lastometrics = {key: value[-1] for key, value in training_history.items()}\n",
    "   phimodel.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   valfinal=phimodel.test_step(datacasted)\n",
    "   valfinal = {key: value.numpy() for key, value in valfinal.items()}\n",
    "   #phimodel.learn_ricci_val=False \n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for raw network: \")\n",
    "   print(valraw)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valfinal)\n",
    "   print(\"ratio of final to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valfinal.items()}))\n",
    "   print(\"ratio of final to raw: \" + str({key + \" ratio\": value/(valraw[key]+1e-8) for key, value in valfinal.items()}))\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(phimodel,tf.cast(data[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   #IMPLEMENT THE FOLLOWING\n",
    "   #meanfailuretosolveequation,_,_=measure_laplacian_failure(phimodel,data)\n",
    "   print(\"\\n\\n\")\n",
    "   return phimodel,training_history\n",
    "\n",
    "def load_nn_phimodel(free_coefficient,nlayer=3,nHidden=128,nEpochs=50,bSizes=[192,50000],stddev=0.1,lRate=0.001,set_weights_to_zero=False):\n",
    "   dirname = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   name = 'phimodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(bSizes[1]) + 's' + str(nlayer) + 'x' +str(nHidden)\n",
    "   print(dirname)\n",
    "   print(name)\n",
    "   \n",
    "   data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirname, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetrics(data)\n",
    "\n",
    "\n",
    "   act = 'gelu'\n",
    "\n",
    "   alpha = [1., 1., 30., 1., 2.] # 1 AND 3??\n",
    " \n",
    "\n",
    "\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   nn_phi = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_phi_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "\n",
    "\n",
    "#    nn_phi = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "#    nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   phimodel = PhiFSModel(nn_phi, BASIS, alpha=alpha)\n",
    "   phimodelzero = PhiFSModel(nn_phi_zero, BASIS, alpha=alpha)\n",
    "\n",
    "   if set_weights_to_zero:\n",
    "      training_history=0\n",
    "   else:\n",
    "      phimodel.model=tf.keras.models.load_model(os.path.join(dirname,name))\n",
    "      training_history=np.load(os.path.join(dirname, 'trainingHistory-' + name +'.npz'),allow_pickle=True)['arr_0'].item()\n",
    "\n",
    "   phimodel.compile(custom_metrics=cmetrics)\n",
    "   phimodelzero.compile(custom_metrics=cmetrics)\n",
    "\n",
    "   # compare validation loss before training for zero network and nonzero network\n",
    "   datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   #need to re-enable learning, in case there's been a problem:\n",
    "   phimodel.learn_transition = True\n",
    "   phimodelzero.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "   phimodelzero.learn_volk = True\n",
    "   #phimodel.learn_ricci_val= True\n",
    "   #phimodelzero.learn_ricci_val= True\n",
    "   valzero=phimodelzero.evaluate(datacasted[0],datacasted[1])\n",
    "   valtrained=phimodel.evaluate(datacasted[0],datacasted[1])\n",
    "   metricsnames=phimodel.metrics_names\n",
    "   # phimodel.learn_ricci_val=False \n",
    "   # phimodelzero.learn_ricci_val=False \n",
    "   valzero = {metricsnames[i]: valzero[i] for i in range(len(valzero))}\n",
    "   valtrained= {metricsnames[i]: valtrained[i] for i in range(len(valtrained))}\n",
    "\n",
    "   #valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   #valtrained = {key: value.numpy() for key, value in valtrained.items()}\n",
    "\n",
    "   #valtrained = {key: value.numpy() for key, value in valtrained.items()}\n",
    "\n",
    "   phimodel.learn_transition = True\n",
    "   phimodel.learn_volk = True\n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valtrained)\n",
    "   print(\"ratio of trained to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valtrained.items()}))\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(phimodel,tf.cast(data[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   print(\"\\n\\n\")\n",
    "   #IMPLEMENT THE FOLLOWING\n",
    "   #meanfailuretosolveequation,_,_=measure_laplacian_failure(phimodel,data)\n",
    "   #print(\"\\n\\n\")\n",
    "   #print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   return phimodel,training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f55f1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "848e584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_points_and_save_using_defaultsAlpha(free_coefficient,phimodel,euler_char,force_generate=False,seed_set=0):\n",
    "   coefficients=coefficientsTQ\n",
    "   # coefficients=np.array([1, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, \\\n",
    "   # 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, free_coefficient, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, \\\n",
    "   # 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1])\n",
    "   monomials=monomialsTQ\n",
    "   kmoduli=kmoduliTQ\n",
    "   ambient=ambientTQ\n",
    "   # monomials = 5*np.eye(5, dtype=np.int64)\n",
    "   # coefficients = np.ones(5)\n",
    "   \n",
    "   # kmoduli = np.ones(1)\n",
    "   # ambient = np.array([4])\n",
    "   pg = PointGenerator(monomials, coefficients, kmoduli, ambient)\n",
    "   pg._set_seed(seed_set)\n",
    "\n",
    "\n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "   #dirnameAlpha = 'dataAlphaP/tetraquadricAlpha_pg_with_'+str(free_coefficient)+'forLB_'+lbstring\n",
    "   #dirnameForMetric = 'dataAlphaP/tetraquadric_pg_with_'+str(free_coefficient)\n",
    "   print(\"dirname for alpha: \" + dirnameForMetric)\n",
    "   print(\"dirname for alpha: \" + dirnameAlpha)\n",
    "\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "   \n",
    "   data=np.load(os.path.join(dirnameForMetric, 'dataset.npz'))\n",
    "\n",
    "   if force_generate or (not os.path.exists(dirnameAlpha)):\n",
    "      print(\"Generating: forced? \" + str(force_generate))\n",
    "      kappaAlpha = prepare_dataset_Alpha(pg,data,dirnameAlpha,phimodel,euler_char,BASIS,normalize_to_vol_j=True);\n",
    "   elif os.path.exists(dirnameAlpha):\n",
    "      try:\n",
    "         print(\"loading prexisting dataset\")\n",
    "         data = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "      except:\n",
    "         print(\"problem loading data - generating anyway\")\n",
    "         kappaAlpha = prepare_dataset_Alpha(pg,data, dirnameAlpha,phimodel,euler_char,BASIS,normalize_to_vol_j=True);\n",
    "      \n",
    "   \n",
    "\n",
    "def getcallbacksandmetricsAlpha(dataalpha):\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   tcb = TransitionCallback((dataalpha['X_val'], dataalpha['y_val']))\n",
    "   lplcb = LaplacianCallback(dataalpha_val_dict)\n",
    "   # lplcb = LaplacianCallback(data_val)\n",
    "   cb_list = [lplcb,tcb]\n",
    "   cmetrics = [TotalLoss(), LaplacianLoss(), TransitionLoss()]\n",
    "   return cb_list, cmetrics\n",
    "\n",
    "   \n",
    "def train_and_save_nn_Alpha(free_coefficient,phimodel,euler_char,alphaprime,nlayer=3,nHidden=128,nEpochs=30,bSizes=[192,50000],stddev=0.1,lRate=0.001,use_zero_network=False,alpha=[1,1],load_network=False):\n",
    "   \n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "\n",
    "   #data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "\n",
    "   dataalpha = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "   dataalpha_train=tf.data.Dataset.from_tensor_slices(dict(list(dict(dataalpha).items())[:len(dict(dataalpha))//2]))\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   dataalpha_val=tf.data.Dataset.from_tensor_slices(dataalpha_val_dict)\n",
    "   # batch_sizes=[64,10000]\n",
    "   dataalpha_train=dataalpha_train.shuffle(buffer_size=1024).batch(bSizes[0])\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 30\n",
    "   #bSizes = [192, 150000]\n",
    "   #alpha = [1., 1.] # 1 AND 3??\n",
    "   nfold = 3\n",
    "   n_in = 2*8\n",
    "   n_out = 1\n",
    "   #lRate = 0.001\n",
    "   name = 'alphamodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(nlayer) + 'x' +str(nHidden)\n",
    "   print(\"name: \" + name)\n",
    "\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   #nfirstlayer=tf.reduce_sum(((np.array(ambient)+1)**2)).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=stddev)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   activ=tf.square\n",
    "   #activ=tfk.activations.gelu\n",
    "   #nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=False)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_alpha_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #copie from phi above\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero =BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.2)\n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network,kernel_initializer=initializer)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   \n",
    "   alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "   if load_network:\n",
    "      print(\"loading network\")\n",
    "      alphamodel.model=tf.keras.models.load_model(os.path.join(dirnameAlpha,name))\n",
    "      print(\"network loaded\")\n",
    "\n",
    "   alphamodelzero= AlphaPrimeModel(nn_alpha_zero,BASIS,phimodel, alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "\n",
    "   #Note, currently running legacy due to ongoing tf issue with M1/M2. \n",
    "   #Use the commented line instead if not on an M1/M2 machine\n",
    "   #opt = tfk.optimizers.Adam(learning_rate=lRate)\n",
    "   opt = tfk.optimizers.legacy.Adam(learning_rate=lRate)\n",
    "   # compile so we can test on validation set before training\n",
    "   alphamodel.compile(custom_metrics=cmetrics)\n",
    "   alphamodelzero.compile(custom_metrics=cmetrics)\n",
    "   \n",
    "   #datacasted=[tf.cast(data['X_val'],tf.float32),tf.cast(data['y_val'],tf.float32)]\n",
    "   valzero=alphamodelzero.test_step(dataalpha_val_dict)\n",
    "   valraw=alphamodel.test_step(dataalpha_val_dict)\n",
    "   valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   valraw = {key: value.numpy() for key, value in valraw.items()}\n",
    "   \n",
    "   training_historyAlpha={'transition_loss': [10**(-8)],'laplacian_loss': [1000000000000000]}\n",
    "   i=0\n",
    "   newLR=lRate\n",
    "   #while (training_historyAlpha['transition_loss'][-1]<10**(-5)) or (training_historyAlpha['laplacian_loss'][-1]>1.):\n",
    "   # continue looping if >10 or is nan\n",
    "   while i==0:#(training_historyAlpha['laplacian_loss'][-1]>10000000000000.) or (np.isnan( training_historyAlpha['laplacian_loss'][-1])):\n",
    "      print(\"trying iteration of training \"+str(i))\n",
    "      if i >0:\n",
    "\n",
    "         print('trying again laplacian_loss too big')\n",
    "         #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.2)\n",
    "         #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network,kernel_initializer=initializer)#note we don't need a last bias (flat direction)\n",
    "         #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = BiholoModelFuncGENERALforAlphainv2(shapeofnetwork,BASIS,activation=tfk.activations.gelu,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev,use_zero_network=use_zero_network)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=False)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "         #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)#note we don't need a last bias (flat direction)\n",
    "         if newLR>0.0002:\n",
    "             newLR=newLR/2\n",
    "             print(\"new LR \" + str(newLR))\n",
    "         opt = tfk.optimizers.legacy.Adam(learning_rate=newLR)\n",
    "         alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "         cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "         alphamodel.compile(custom_metrics=cmetrics)\n",
    "      alphamodel, training_historyAlpha= train_modelalpha(alphamodel, dataalpha_train, optimizer=opt, epochs=nEpochs, batch_sizes=bSizes, \n",
    "                                        verbose=1, custom_metrics=cmetrics, callbacks=cb_list)\n",
    "      i+=1\n",
    "   print(\"finished training\\n\")\n",
    "   alphamodel.model.save(os.path.join(dirnameAlpha, name))\n",
    "   np.savez_compressed(os.path.join(dirnameAlpha, 'trainingHistory-' + name),training_historyAlpha)\n",
    "   valfinal =alphamodel.test_step(dataalpha_val_dict)\n",
    "   valfinal = {key: value.numpy() for key, value in valfinal.items()}\n",
    "   #return training_historyAlpha\n",
    "   #now print the initial losses and final losses for each metric, by taking the first element of each key in the dictionary\n",
    "   #first_metrics = {key: value[0] for key, value in training_historyAlpha.items()}\n",
    "   #last_metrics = {key: value[-1] for key, value in training_historyAlpha.items()}\n",
    "\n",
    "   #print(\"initial losses\")\n",
    "   #print(first_metrics)\n",
    "   #print(\"final losses\")\n",
    "   #print(last_metrics)\n",
    "\n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for raw network: \")\n",
    "   print(valraw)\n",
    "   print(\"validation loss for final network: \")\n",
    "   print(valfinal)\n",
    "   print(\"ratio of final to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valfinal.items()}))\n",
    "   print(\"ratio of final to raw: \" + str({key + \" ratio\": value/(valraw[key]+1e-8) for key, value in valfinal.items()}))\n",
    "\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(alphamodel,tf.cast(dataalpha[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   meanfailuretosolveequation,_,_=HYM_measure_val(alphamodel,dataalpha)\n",
    "   print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   print(\"\\n\\n\")\n",
    "   tf.keras.backend.clear_session()\n",
    "   return alphamodel,training_historyAlpha\n",
    "\n",
    "def load_nn_Alpha(free_coefficient,phimodel,euler_char,alphaprime,nlayer=3,nHidden=128,nEpochs=30,bSizes=[192,50000],stddev=0.1,lRate=0.001,use_zero_network=False,alpha=[1,1],load_network=False):\n",
    "   dirnameForMetric = 'dataAlphaP/'+nameofmanifold+ \"_pg_with_\" + str(free_coefficient) \n",
    "   dirnameAlpha= 'dataAlphaP/'+nameofmanifold+ \"Alpha_pg_with_\" + str(free_coefficient) \n",
    "\n",
    "   name = 'alphamodel_for_' + str(nEpochs) + '_' + str(bSizes[0]) + '_'+ str(nlayer) + 'x' +str(nHidden)\n",
    "   print(\"name: \" + name)\n",
    "\n",
    "   #data = np.load(os.path.join(dirname, 'dataset.npz'))\n",
    "   BASIS = prepare_tf_basis(np.load(os.path.join(dirnameForMetric, 'basis.pickle'), allow_pickle=True))\n",
    "\n",
    "\n",
    "   dataalpha = np.load(os.path.join(dirnameAlpha, 'dataset.npz'))\n",
    "   dataalpha_train=tf.data.Dataset.from_tensor_slices(dict(list(dict(dataalpha).items())[:len(dict(dataalpha))//2]))\n",
    "   dataalpha_val_dict=dict(list(dict(dataalpha).items())[len(dict(dataalpha))//2:])\n",
    "   dataalpha_val=tf.data.Dataset.from_tensor_slices(dataalpha_val_dict)\n",
    "   # batch_sizes=[64,10000]\n",
    "   dataalpha_train=dataalpha_train.shuffle(buffer_size=1024).batch(bSizes[0])\n",
    "\n",
    "   cb_list, cmetrics = getcallbacksandmetricsAlpha(dataalpha)\n",
    "\n",
    "   #nlayer = 3\n",
    "   #nHidden = 128\n",
    "   act = 'gelu'\n",
    "   #nEpochs = 30\n",
    "   #bSizes = [192, 150000]\n",
    "   alpha = [1., 1.] # 1 AND 3??\n",
    "   nfold = 3\n",
    "   n_in = 2*8\n",
    "   n_out = 1\n",
    "   #lRate = 0.001\n",
    "   ambient=tf.cast(BASIS['AMBIENT'],tf.int32)\n",
    "\n",
    "   nfirstlayer=tf.reduce_prod((np.array(ambient)+1)**2).numpy().item()\n",
    "   shapeofinternalnetwork=[nHidden]*nlayer\n",
    "   shapeofnetwork=[nfirstlayer]+shapeofinternalnetwork+[1]\n",
    "\n",
    "   print(\"network shape: \" + str(shapeofnetwork))\n",
    "   #initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=stddev)\n",
    "   #nn_alpha = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,stddev=stddev,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #nn_alpha_zero = BiholoModelFuncGENERAL(shapeofnetwork,BASIS,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   activ=tf.square\n",
    "   nn_alpha = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,stddev=stddev)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   nn_alpha_zero = BiholoModelFuncGENERALforHYMinv3(shapeofnetwork,BASIS,activation=activ,use_zero_network=True)#make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=use_zero_network)\n",
    "   #copie from phi above\n",
    "   #nn_phi_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)\n",
    "   \n",
    "   #nn_alpha = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   #nn_alpha_zero = make_nn(n_in,n_out,nlayer,nHidden,act,use_zero_network=True)#note we don't need a last bias (flat direction)\n",
    "   alphamodel= AlphaPrimeModel(nn_alpha,BASIS, phimodel,alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "   alphamodelzero= AlphaPrimeModel(nn_alpha_zero,BASIS,phimodel, alphaprime,euler_char,alpha=alpha,norm = [1. for _ in range(2)])\n",
    "\n",
    "   if set_weights_to_zero:\n",
    "      training_historyAlpha=0\n",
    "   else:\n",
    "      alphamodel.model=tf.keras.models.load_model(os.path.join(dirnameAlpha,name))\n",
    "      training_historyAlpha=np.load(os.path.join(dirnameAlpha, 'trainingHistory-' + name +'.npz'),allow_pickle=True)['arr_0'].item()\n",
    "\n",
    "   alphamodel.compile(custom_metrics=cmetrics)\n",
    "   alphamodelzero.compile(custom_metrics=cmetrics)\n",
    "   \n",
    "   valzero=alphamodelzero.evaluate(dataalpha_val_dict)\n",
    "   valtrained=alphamodel.evaluate(dataalpha_val_dict)\n",
    "   #valzero = {key: value.numpy() for key, value in valzero.items()}\n",
    "   #valtrained= {key: value.numpy() for key, value in valtrained.items()}\n",
    "\n",
    "\n",
    "   metricsnames=alphamodel.metrics_names\n",
    "\n",
    "   valzero = {metricsnames[i]: valzero[i] for i in range(len(valzero))}\n",
    "   valtrained= {metricsnames[i]: valtrained[i] for i in range(len(valtrained))}\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "   print(\"zero network validation loss: \")\n",
    "   print(valzero)\n",
    "   print(\"validation loss for trained network: \")\n",
    "   print(valtrained)\n",
    "   print(\"ratio of trained to zero: \" + str({key + \" ratio\": value/(valzero[key]+1e-8) for key, value in valtrained.items()}))\n",
    "\n",
    "\n",
    "   averagediscrepancyinstdevs,_=compute_transition_pointwise_measure(alphamodel,tf.cast(dataalpha[\"X_val\"],tf.float32))\n",
    "   print(\"average transition discrepancy in standard deviations: \" + str(averagediscrepancyinstdevs))\n",
    "   meanfailuretosolveequation,_,_=HYM_measure_val(alphamodel,dataalpha)\n",
    "   print(\"mean of difference/mean of absolute value of source, weighted by sqrt(g): \" + str(meanfailuretosolveequation))\n",
    "   print(\"\\n\\n\")\n",
    "   return alphamodel,training_historyAlpha\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb10fbce",
   "metadata": {},
   "source": [
    "Now generate example points with a point generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a2a6fe4",
   "metadata": {},
   "source": [
    "Geneate the point cloud for our NN training - note that this will take a few mins\n",
    "\n",
    "\n",
    "Note that \"free_coefficient\" is just a label for this particular quintic - for the TQ it was psi. Here, it just lets you have different runs not overwrite each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a21f4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirname: dataAlphaP/Quintic_pg_with_2.342343234\n",
      "loading prexisting dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nPoints=300000\n",
    "\n",
    "free_coefficient = 1.9#float(sys.argv[1])\n",
    "free_coefficient=2.342351\n",
    "free_coefficient=2.342343234\n",
    "#free_coefficient=1.# when the coefficient is 1, ensure that it's 1., not 1 for the sake of the filename\n",
    "#nEpochsPhi=100\n",
    "nEpochsPhi=1\n",
    "\n",
    "depthPhi=3\n",
    "widthPhi=128#128 4 in the 1.0s\n",
    "\n",
    "\n",
    "train_phi=True\n",
    "generate_points_and_save_using_defaults(free_coefficient,nPoints)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01658c4c",
   "metadata": {},
   "source": [
    "## Training the NN\n",
    "\n",
    "Now we can start preperation for training the NN\n",
    "\n",
    "Begin by loading in the required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd5dbc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirname: dataAlphaP/Quintic_pg_with_2.342343234\n",
      "name: phimodel_for_10_64_50000s3x128\n",
      "network shape: [25, 128, 128, 128, 1]\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "Epoch  1/10\n",
      "1073/1407 [=====================>........] - ETA: 2s - loss: 0.1029 - sigma_loss: 0.1029 - kaehler_loss: 0.0000e+00 - transition_loss: 0.0000e+00 - volk_loss: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_phi:\n\u001b[0;32m----> 2\u001b[0m     phimodel1,training_history\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_and_save_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfree_coefficient\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepthPhi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwidthPhi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnEpochsPhi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstddev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbSizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlRate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     phimodel1,training_history\u001b[38;5;241m=\u001b[39mload_nn_phimodel(free_coefficient,depthPhi,widthPhi,nEpochsPhi,[\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m50000\u001b[39m],set_weights_to_zero\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 111\u001b[0m, in \u001b[0;36mtrain_and_save_nn\u001b[0;34m(free_coefficient, nlayer, nHidden, nEpochs, stddev, bSizes, lRate, use_zero_network)\u001b[0m\n\u001b[1;32m    108\u001b[0m valzero \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m valzero\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    109\u001b[0m valraw \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m valraw\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 111\u001b[0m phimodel, training_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphimodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnEpochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbSizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinished training\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m phimodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, name))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MLenv/lib/python3.10/site-packages/cymetric/models/tfhelper.py:75\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(fsmodel, data, optimizer, epochs, batch_sizes, verbose, custom_metrics, callbacks, sw)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{:2d}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{:d}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, epochs))\n\u001b[0;32m---> 75\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mfsmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m history\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m hist1\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MLenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MLenv/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MLenv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MLenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MLenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MLenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MLenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MLenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MLenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MLenv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/MLenv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if train_phi:\n",
    "    phimodel1,training_history=train_and_save_nn(free_coefficient,depthPhi,widthPhi,nEpochsPhi,stddev=0.05,bSizes=[64,50000],lRate=0.001) \n",
    "else:\n",
    "    phimodel1,training_history=load_nn_phimodel(free_coefficient,depthPhi,widthPhi,nEpochsPhi,[64,50000],set_weights_to_zero=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1eb5fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirname for alpha: dataAlphaP/Quintic_pg_with_2.342343234\n",
      "dirname for alpha: dataAlphaP/QuinticAlpha_pg_with_2.342343234\n",
      "loading prexisting dataset\n"
     ]
    }
   ],
   "source": [
    "generate_points_and_save_using_defaultsAlpha(free_coefficient,phimodel1,-200.,force_generate=False,seed_set=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a058210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: alphamodel_for_100_64_3x128\n",
      "network shape: [25, 128, 128, 128, 1]\n",
      "trying iteration of training 0\n",
      "\n",
      "Epoch  1/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 1194.3726 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 60.0688\n",
      " - Transition measure val: 4.1232e-07\n",
      "1407/1407 [==============================] - 17s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 1192.8094 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 60.0688 - transition_val: 4.1232e-07\n",
      "\n",
      "Epoch  2/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 43.2649 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 39.3161\n",
      " - Transition measure val: 5.4331e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 43.2469 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 39.3161 - transition_val: 5.4331e-07\n",
      "\n",
      "Epoch  3/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 39.7710 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 38.7680\n",
      " - Transition measure val: 5.2767e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 39.7810 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 38.7680 - transition_val: 5.2767e-07\n",
      "\n",
      "Epoch  4/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 38.9425 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 38.2296\n",
      " - Transition measure val: 5.2376e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 38.9452 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 38.2296 - transition_val: 5.2376e-07\n",
      "\n",
      "Epoch  5/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 38.6760 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 38.1248\n",
      " - Transition measure val: 5.4407e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 38.6760 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 38.1248 - transition_val: 5.4407e-07\n",
      "\n",
      "Epoch  6/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 38.2059 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 37.1304\n",
      " - Transition measure val: 5.3329e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 38.2230 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 37.1304 - transition_val: 5.3329e-07\n",
      "\n",
      "Epoch  7/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 38.1079 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 36.8252\n",
      " - Transition measure val: 5.3635e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 38.1037 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 36.8252 - transition_val: 5.3635e-07\n",
      "\n",
      "Epoch  8/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 37.9002 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 37.1113\n",
      " - Transition measure val: 5.1861e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 37.9143 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 37.1113 - transition_val: 5.1861e-07\n",
      "\n",
      "Epoch  9/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 37.6442 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 36.4164\n",
      " - Transition measure val: 5.5342e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 37.6266 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 36.4164 - transition_val: 5.5342e-07\n",
      "\n",
      "Epoch 10/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 37.5376 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 36.4851\n",
      " - Transition measure val: 5.5180e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 37.5184 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 36.4851 - transition_val: 5.5180e-07\n",
      "\n",
      "Epoch 11/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 37.4302 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 36.9850\n",
      " - Transition measure val: 5.4312e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 37.4149 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 36.9850 - transition_val: 5.4312e-07\n",
      "cutting LR, multiplying by 0.1 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.010000001>\n",
      "\n",
      "Epoch 12/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 35.5868 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5914\n",
      " - Transition measure val: 5.0907e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 35.6174 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5914 - transition_val: 5.0907e-07\n",
      "\n",
      "Epoch 13/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 35.1380 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5058\n",
      " - Transition measure val: 5.2481e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 35.1270 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5058 - transition_val: 5.2481e-07\n",
      "\n",
      "Epoch 14/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 35.0545 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.3880\n",
      " - Transition measure val: 5.3854e-07\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 35.0717 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.3880 - transition_val: 5.3854e-07\n",
      "\n",
      "Epoch 15/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.9726 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4474\n",
      " - Transition measure val: 5.2948e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 35.0310 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4474 - transition_val: 5.2948e-07\n",
      "\n",
      "Epoch 16/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.9413 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4761\n",
      " - Transition measure val: 5.2032e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.9628 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4761 - transition_val: 5.2032e-07\n",
      "\n",
      "Epoch 17/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.9239 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4593\n",
      " - Transition measure val: 5.2099e-07\n",
      "1407/1407 [==============================] - 14s 9ms/step - loss: 0.0000e+00 - laplacian_loss: 34.9077 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4593 - transition_val: 5.2099e-07\n",
      "\n",
      "Epoch 18/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.8378 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5509\n",
      " - Transition measure val: 5.2757e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.8260 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5509 - transition_val: 5.2757e-07\n",
      "\n",
      "Epoch 19/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.7944 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5710\n",
      " - Transition measure val: 5.3511e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.7679 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5710 - transition_val: 5.3511e-07\n",
      "\n",
      "Epoch 20/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.7454 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.6484\n",
      " - Transition measure val: 5.2872e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.7337 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.6484 - transition_val: 5.2872e-07\n",
      "\n",
      "Epoch 21/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.6672 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.6334\n",
      " - Transition measure val: 5.3444e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.7112 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.6334 - transition_val: 5.3444e-07\n",
      "cutting LR, multiplying by 0.1 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "\n",
      "Epoch 22/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.3926 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5442\n",
      " - Transition measure val: 5.3740e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.3784 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5442 - transition_val: 5.3740e-07\n",
      "\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.3459 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5079\n",
      " - Transition measure val: 5.1575e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.3459 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5079 - transition_val: 5.1575e-07\n",
      "\n",
      "Epoch 24/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.3326 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4923\n",
      " - Transition measure val: 5.2958e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.3284 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4923 - transition_val: 5.2958e-07\n",
      "\n",
      "Epoch 25/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.3092 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4823\n",
      " - Transition measure val: 5.2996e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.3127 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4823 - transition_val: 5.2996e-07\n",
      "\n",
      "Epoch 26/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.3121 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4805\n",
      " - Transition measure val: 5.3682e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2971 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4805 - transition_val: 5.3682e-07\n",
      "\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2853 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4934\n",
      " - Transition measure val: 5.3425e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2853 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4934 - transition_val: 5.3425e-07\n",
      "\n",
      "Epoch 28/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2778 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.4992\n",
      " - Transition measure val: 5.2557e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2713 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.4992 - transition_val: 5.2557e-07\n",
      "\n",
      "Epoch 29/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2865 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5074\n",
      " - Transition measure val: 5.1708e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2659 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5074 - transition_val: 5.1708e-07\n",
      "\n",
      "Epoch 30/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2712 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5123\n",
      " - Transition measure val: 5.1098e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2567 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5123 - transition_val: 5.1098e-07\n",
      "\n",
      "Epoch 31/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2643 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5145\n",
      " - Transition measure val: 5.3902e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2549 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5145 - transition_val: 5.3902e-07\n",
      "cutting LR, multiplying by 0.1 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.000100000005>\n",
      "\n",
      "Epoch 32/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2176 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5104\n",
      " - Transition measure val: 5.3139e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2123 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5104 - transition_val: 5.3139e-07\n",
      "\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2137 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5064\n",
      " - Transition measure val: 5.1813e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2137 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5064 - transition_val: 5.1813e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=9e-05>\n",
      "\n",
      "Epoch 34/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2032 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5033\n",
      " - Transition measure val: 5.3396e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2596 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5033 - transition_val: 5.3396e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=8.1e-05>\n",
      "\n",
      "Epoch 35/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2027 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5024\n",
      " - Transition measure val: 5.2881e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1982 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5024 - transition_val: 5.2881e-07\n",
      "\n",
      "Epoch 36/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2002 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5019\n",
      " - Transition measure val: 5.2319e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2125 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5019 - transition_val: 5.2319e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=7.2899995e-05>\n",
      "\n",
      "Epoch 37/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2278 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5022\n",
      " - Transition measure val: 5.2023e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1954 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5022 - transition_val: 5.2023e-07\n",
      "\n",
      "Epoch 38/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2071 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5016\n",
      " - Transition measure val: 5.3692e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1954 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5016 - transition_val: 5.3692e-07\n",
      "\n",
      "Epoch 39/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1776 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5012\n",
      " - Transition measure val: 5.4293e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1938 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5012 - transition_val: 5.4293e-07\n",
      "\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1955 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5020\n",
      " - Transition measure val: 5.4741e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1955 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5020 - transition_val: 5.4741e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=6.5609995e-05>\n",
      "\n",
      "Epoch 41/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1768 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5019\n",
      " - Transition measure val: 5.2032e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1898 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5019 - transition_val: 5.2032e-07\n",
      "\n",
      "Epoch 42/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2021 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5030\n",
      " - Transition measure val: 5.3263e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1904 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5030 - transition_val: 5.3263e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=5.9048994e-05>\n",
      "\n",
      "Epoch 43/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1570 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5023\n",
      " - Transition measure val: 5.2814e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2676 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5023 - transition_val: 5.2814e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=5.3144093e-05>\n",
      "\n",
      "Epoch 44/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2098 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5019\n",
      " - Transition measure val: 5.2166e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1914 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5019 - transition_val: 5.2166e-07\n",
      "\n",
      "Epoch 45/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1731 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5020\n",
      " - Transition measure val: 5.2681e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2052 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5020 - transition_val: 5.2681e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=4.7829682e-05>\n",
      "\n",
      "Epoch 46/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2112 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5025\n",
      " - Transition measure val: 5.3883e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1908 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5025 - transition_val: 5.3883e-07\n",
      "\n",
      "Epoch 47/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1446 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5025\n",
      " - Transition measure val: 5.2614e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1860 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5025 - transition_val: 5.2614e-07\n",
      "\n",
      "Epoch 48/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1956 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5025\n",
      " - Transition measure val: 5.2509e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1877 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5025 - transition_val: 5.2509e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=4.304671e-05>\n",
      "\n",
      "Epoch 49/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1866 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5027\n",
      " - Transition measure val: 5.2385e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1841 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5027 - transition_val: 5.2385e-07\n",
      "\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1916 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5032\n",
      " - Transition measure val: 5.0335e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1916 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5032 - transition_val: 5.0335e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=3.874204e-05>\n",
      "\n",
      "Epoch 51/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2169 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5038\n",
      " - Transition measure val: 5.3892e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1854 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5038 - transition_val: 5.3892e-07\n",
      "\n",
      "Epoch 52/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1937 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5037\n",
      " - Transition measure val: 5.3301e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1878 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5037 - transition_val: 5.3301e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=3.4867837e-05>\n",
      "\n",
      "Epoch 53/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.2134 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5037\n",
      " - Transition measure val: 5.2605e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1840 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5037 - transition_val: 5.2605e-07\n",
      "\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1828 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5041\n",
      " - Transition measure val: 5.2700e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1828 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5041 - transition_val: 5.2700e-07\n",
      "\n",
      "Epoch 55/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1921 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5044\n",
      " - Transition measure val: 5.2795e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1875 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5044 - transition_val: 5.2795e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=3.1381052e-05>\n",
      "\n",
      "Epoch 56/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1936 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5042\n",
      " - Transition measure val: 5.3511e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1814 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5042 - transition_val: 5.3511e-07\n",
      "\n",
      "Epoch 57/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1806 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5047\n",
      " - Transition measure val: 5.1708e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2207 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5047 - transition_val: 5.1708e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=2.8242946e-05>\n",
      "\n",
      "Epoch 58/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1758 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5042\n",
      " - Transition measure val: 5.3072e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2319 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5042 - transition_val: 5.3072e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=2.541865e-05>\n",
      "\n",
      "Epoch 59/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1968 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5044\n",
      " - Transition measure val: 5.1994e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1968 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5044 - transition_val: 5.1994e-07\n",
      "\n",
      "Epoch 60/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1920 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5045\n",
      " - Transition measure val: 5.3549e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1801 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5045 - transition_val: 5.3549e-07\n",
      "\n",
      "Epoch 61/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1760 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5044\n",
      " - Transition measure val: 5.3558e-07\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2044 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5044 - transition_val: 5.3558e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=2.2876786e-05>\n",
      "\n",
      "Epoch 62/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1638 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5051\n",
      " - Transition measure val: 5.3997e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1918 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5051 - transition_val: 5.3997e-07\n",
      "\n",
      "Epoch 63/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1906 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5050\n",
      " - Transition measure val: 5.3997e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1797 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5050 - transition_val: 5.3997e-07\n",
      "\n",
      "Epoch 64/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1689 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5049\n",
      " - Transition measure val: 5.1241e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1876 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5049 - transition_val: 5.1241e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=2.0589106e-05>\n",
      "\n",
      "Epoch 65/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1700 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5050\n",
      " - Transition measure val: 5.3778e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1776 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5050 - transition_val: 5.3778e-07\n",
      "\n",
      "Epoch 66/100\n",
      "1402/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1830 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5048\n",
      " - Transition measure val: 5.5208e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2621 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5048 - transition_val: 5.5208e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=1.8530196e-05>\n",
      "\n",
      "Epoch 67/100\n",
      "1403/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1525 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5048\n",
      " - Transition measure val: 5.1966e-07\n",
      "1407/1407 [==============================] - 16s 10ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1766 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5048 - transition_val: 5.1966e-07\n",
      "\n",
      "Epoch 68/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1899 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5051\n",
      " - Transition measure val: 5.3139e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1760 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5051 - transition_val: 5.3139e-07\n",
      "\n",
      "Epoch 69/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1849 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5052\n",
      " - Transition measure val: 5.3883e-07\n",
      "1407/1407 [==============================] - 19s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1778 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5052 - transition_val: 5.3883e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=1.6677175e-05>\n",
      "\n",
      "Epoch 70/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1852 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5052\n",
      " - Transition measure val: 5.2929e-07\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1893 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5052 - transition_val: 5.2929e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=1.50094575e-05>\n",
      "\n",
      "Epoch 71/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1858 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5053\n",
      " - Transition measure val: 5.3320e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1868 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5053 - transition_val: 5.3320e-07\n",
      "\n",
      "Epoch 72/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1796 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5053\n",
      " - Transition measure val: 5.0678e-07\n",
      "1407/1407 [==============================] - 17s 11ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1764 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5053 - transition_val: 5.0678e-07\n",
      "\n",
      "Epoch 73/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1557 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5054\n",
      " - Transition measure val: 5.2824e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1781 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5054 - transition_val: 5.2824e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=1.3508511e-05>\n",
      "\n",
      "Epoch 74/100\n",
      "1405/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1772 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5052\n",
      " - Transition measure val: 5.3406e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1973 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5052 - transition_val: 5.3406e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=1.21576595e-05>\n",
      "\n",
      "Epoch 75/100\n",
      "1407/1407 [==============================] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1743 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5052\n",
      " - Transition measure val: 5.2900e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1743 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5052 - transition_val: 5.2900e-07\n",
      "\n",
      "Epoch 76/100\n",
      "1406/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1719 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5052\n",
      " - Transition measure val: 5.2128e-07\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.0000e+00 - laplacian_loss: 34.2235 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5052 - transition_val: 5.2128e-07\n",
      "cutting LR, multiplying by 0.9 - new LR: <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=1.0941893e-05>\n",
      "\n",
      "Epoch 77/100\n",
      "1404/1407 [============================>.] - ETA: 0s - loss: 0.0000e+00 - laplacian_loss: 34.1951 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - Laplacian measure val: 34.5053\n",
      " - Transition measure val: 5.3177e-07\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 0.0000e+00 - laplacian_loss: 34.1754 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00 - laplacian_val: 34.5053 - transition_val: 5.3177e-07\n",
      "\n",
      "Epoch 78/100\n",
      "  61/1407 [>.............................] - ETA: 9s - loss: 0.0000e+00 - laplacian_loss: 30.2790 - transition_loss: 0.0000e+00 - sigma_loss: 0.0000e+00 - kaehler_loss: 0.0000e+00 - volk_loss: 0.0000e+00"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node gradients/AddN_2 defined at (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n\n  File \"/var/folders/n5/4xlf9f2j41z1r42htkz4vw0m0000gn/T/ipykernel_91769/2454447764.py\", line 7, in <module>\n\n  File \"/var/folders/n5/4xlf9f2j41z1r42htkz4vw0m0000gn/T/ipykernel_91769/2703670363.py\", line 157, in train_and_save_nn_Alpha\n\n  File \"/Users/kit/Documents/Phys_Working/PhysicalYukawas/AlphaPrimeModel.py\", line 587, in train_modelalpha\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/Users/kit/Documents/Phys_Working/PhysicalYukawas/AlphaPrimeModel.py\", line 330, in train_step\n\nInputs to operation PartitionedCall_5/gradients/AddN_2 of type AddN must have the same size and shape.  Input 0: [10,64,5] != input 1: [0]\n\t [[{{node gradients/AddN_2}}]] [Op:__inference_train_function_1724362]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m nEpochsAlpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     AlphaModel1,training_historyAlpha\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_and_save_nn_Alpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfree_coefficient\u001b[49m\u001b[43m,\u001b[49m\u001b[43mphimodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43meuler_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43malphaprime\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepthAlpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwidthAlpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnEpochsAlpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbSizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstddev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlRate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43muse_zero_network\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mload_network\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     AlphaModel1,training_historyAlpha\u001b[38;5;241m=\u001b[39mload_nn_Alpha(free_coefficient,phimodel1,euler_char,alphaprime,depthPhi,widthPhi,nEpochsPhi,[\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m50000\u001b[39m],set_weights_to_zero\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 157\u001b[0m, in \u001b[0;36mtrain_and_save_nn_Alpha\u001b[0;34m(free_coefficient, phimodel, euler_char, alphaprime, nlayer, nHidden, nEpochs, bSizes, stddev, lRate, use_zero_network, alpha, load_network)\u001b[0m\n\u001b[1;32m    155\u001b[0m       cb_list, cmetrics \u001b[38;5;241m=\u001b[39m getcallbacksandmetricsAlpha(dataalpha)\n\u001b[1;32m    156\u001b[0m       alphamodel\u001b[38;5;241m.\u001b[39mcompile(custom_metrics\u001b[38;5;241m=\u001b[39mcmetrics)\n\u001b[0;32m--> 157\u001b[0m    alphamodel, training_historyAlpha\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_modelalpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43malphamodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataalpha_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnEpochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbSizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m    i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinished training\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Phys_Working/PhysicalYukawas/AlphaPrimeModel.py:587\u001b[0m, in \u001b[0;36mtrain_modelalpha\u001b[0;34m(alphaprimemodel, data_train, optimizer, epochs, batch_sizes, verbose, custom_metrics, callbacks, sw)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{:2d}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{:d}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, epochs))\n\u001b[0;32m--> 587\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43malphaprimemodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m#print(history)\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m history\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node gradients/AddN_2 defined at (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n\n  File \"/var/folders/n5/4xlf9f2j41z1r42htkz4vw0m0000gn/T/ipykernel_91769/2454447764.py\", line 7, in <module>\n\n  File \"/var/folders/n5/4xlf9f2j41z1r42htkz4vw0m0000gn/T/ipykernel_91769/2703670363.py\", line 157, in train_and_save_nn_Alpha\n\n  File \"/Users/kit/Documents/Phys_Working/PhysicalYukawas/AlphaPrimeModel.py\", line 587, in train_modelalpha\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/cymetric/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/Users/kit/Documents/Phys_Working/PhysicalYukawas/AlphaPrimeModel.py\", line 330, in train_step\n\nInputs to operation PartitionedCall_5/gradients/AddN_2 of type AddN must have the same size and shape.  Input 0: [10,64,5] != input 1: [0]\n\t [[{{node gradients/AddN_2}}]] [Op:__inference_train_function_1724362]"
     ]
    }
   ],
   "source": [
    "alphaprime=1\n",
    "euler_char=-200\n",
    "depthAlpha=3\n",
    "widthAlpha=128\n",
    "nEpochsAlpha=100\n",
    "\n",
    "\n",
    "train_alpha=True\n",
    "if train_alpha:\n",
    "    AlphaModel1,training_historyAlpha=train_and_save_nn_Alpha(free_coefficient,phimodel1,euler_char,alphaprime,depthAlpha,widthAlpha,nEpochsAlpha,bSizes=[64,50000],stddev=0.05,lRate=0.1,use_zero_network=False,alpha=[1.,1.],load_network=False)\n",
    "else:\n",
    "    AlphaModel1,training_historyAlpha=load_nn_Alpha(free_coefficient,phimodel1,euler_char,alphaprime,depthPhi,widthPhi,nEpochsPhi,[64,50000],set_weights_to_zero=False)\n",
    "\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cymetric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
